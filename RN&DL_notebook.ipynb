{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNxhWvjUyUds6JWTmrP33M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldmontibeller/mestrado/blob/main/RN%26DL_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2025-04-10\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OMY0u-ejsbES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dúvidas\n",
        "*   Perceptron e Adaline\n",
        "*   Regressão Linear e Regressão Logística\n",
        "*   Diferenças nas funções de ativação\n",
        "*   Função objetivo? MSE e binary crossentropy\n",
        "*   Por que usamos derivadas para encontrar o erro?\n"
      ],
      "metadata": {
        "id": "ljxVVrFjvSuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perceptron e Adaline\n",
        "Perceptron: Encontra qualquer reta que que classifica dois grupos\n",
        "\n",
        "Adaline: Econtra a melhor reta dada a função objetivo (redução de custo)"
      ],
      "metadata": {
        "id": "MA2SqFtJv5Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressão linear\n",
        "\n",
        "Regressão linear é um perceptron com uma função de ativação linear $\\sigma$(x) = x, ou seja é uma função identidade que passa o valor computado na net input."
      ],
      "metadata": {
        "id": "2G-gA5Zdxn79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss\n",
        "The loss function is a mathematical function that compares the predicted output to the actual output.\n",
        "The loss function's output is used to adjust the model's weights.\n",
        "A gente faz iterações até o modelo não conseguir mais diminuir o loss. Ou seja, tenta minimizar o loss. E é por isso que utilizamos **derivadas** para tentar achar o ponto onde a derivada do erro é mínima/zero. Alguns exemplos são sum squared errors, mean squared error, etc. Ou seja, você precisa especificar a métrica que está tentando minimizar."
      ],
      "metadata": {
        "id": "p-XShEKVJQex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning rate\n",
        "Taxa de aprendizagem: é apenas um multiplicador η que diz a velocidade que o erro (gradiente negativo do loss) é aprimorado. Fator de escalamento do erro. Existe uma necessidade de sintonizar (tune) a taxa de aprendizado pra ver qual performa melhor (converge mais rápido sem desestabilizar)."
      ],
      "metadata": {
        "id": "82Ll1816KqK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bias\n",
        "Enquanto no perceptron o bias fazia um threshold que definia quando o neurônio era ativado ou não (ativado ou não é uma função de ativação binária), no caso de outras funções de ativação ele desloca essa condição para a função inteira (tanh, ReLU, sigmoid). Sem o bias, o perceptron daria 0 quando fosse menor ou igual a zero e 1 quando fosse maior que um, utilizando o bias podemos mudar para 1,5 (caso AND) ou 0.5 (caso OR)"
      ],
      "metadata": {
        "id": "zuU6AF8qTaZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_z4aozin8p7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}